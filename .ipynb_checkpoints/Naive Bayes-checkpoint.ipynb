{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "Dataset has been downloaded from the UCI machine learning repository that contains several Youtube comments from very popular music videos. Each comment in the data has been labeled as either spam or ham and this data will be used to train Naive Bayes algorithm for youtube comment spam classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "# For matrix operations\n",
    "import numpy as np\n",
    "# For regular expression (text cleaning)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+447935454150 lovely girl talk to me xxx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I always end up coming back to this song&lt;br /&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my sister just received over 6,500 new &lt;a rel=...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cool</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello I am from Palastine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0           +447935454150 lovely girl talk to me xxx      1\n",
       "1     I always end up coming back to this song<br />      0\n",
       "2  my sister just received over 6,500 new <a rel=...      1\n",
       "3                                               Cool      0\n",
       "4                          Hello I am from Palastine      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data set\n",
    "data_comments = pd.read_csv('YoutubeCommentsSpam.csv')\n",
    "\n",
    "# Create column labels\n",
    "data_comments.columns = [\"content\",\"label\"]\n",
    "data_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                +447935454150 lovely girl talk to me xxx\n",
      "2       my sister just received over 6,500 new <a rel=...\n",
      "4                               Hello I am from Palastine\n",
      "6       Go check out my rapping video called Four Whee...\n",
      "8                           Aslamu Lykum... From Pakistan\n",
      "10                            Help me get 50 subs please \n",
      "12      Alright ladies, if you like this song, then ch...\n",
      "15      <a href=\"https://www.facebook.com/groups/10087...\n",
      "16                  Take a look at this video on YouTube:\n",
      "17                 Check out our Channel for nice Beats!!\n",
      "19                    Check out this playlist on YouTube:\n",
      "21                                            like please\n",
      "24      I shared my first song &quot;I Want You&quot;,...\n",
      "25      Come and check out my music!Im spamming on loa...\n",
      "26                    Check out this playlist on YouTube:\n",
      "27      HUH HYUCK HYUCK IM SPECIAL WHO S WATCHING THIS...\n",
      "30      Check out this video on YouTube:<br /><br />Lo...\n",
      "33                    Check out this playlist on YouTube:\n",
      "34                       Check out this video on YouTube:\n",
      "35                       Check out this video on YouTube:\n",
      "38      Check out this playlist on YouTube:chcfcvzfzfb...\n",
      "39                   Check out this playlist on YouTube: \n",
      "40      Im gonna share a little ryhme canibus blows em...\n",
      "41                       Check out this video on YouTube:\n",
      "42      Check out this video on YouTube<br /><br /><br />\n",
      "43        CHECK OUT THE NEW REMIX !!!<br />CLICK CLICK !!\n",
      "44                    Check out this playlist on YouTube:\n",
      "45      I personally have never been in a abusive rela...\n",
      "48                                  plese subscribe to me\n",
      "49                       Check out this video on YouTube:\n",
      "                              ...                        \n",
      "1915             CHECK OUT partyman318 FR GOOD TUNEZ!! :D\n",
      "1916    Hey youtubers... I really appreciate all of yo...\n",
      "1917    Hey Music Fans I really appreciate any of you ...\n",
      "1918    Hey Music Fans I really appreciate any of you ...\n",
      "1919    Hey Music Fans I really appreciate any of you ...\n",
      "1920                   Hi. Check out and share our songs.\n",
      "1921                   Hi. Check out and share our songs.\n",
      "1922                    Hi.Check out and share our songs.\n",
      "1923    Hey Music Fans I really appreciate any of you ...\n",
      "1924    Hey, I am doing the Forty Hour famine so I ll ...\n",
      "1925             Love itt and ppl check out my channel!!!\n",
      "1926                                 SUBSCRIBE MY CHANNEL\n",
      "1927                                       adf.ly / KlD3Y\n",
      "1928                                       adf.ly / KlD3Y\n",
      "1929                               check out my new video\n",
      "1930    Hey Music Fans I really appreciate all of you ...\n",
      "1931    Hello everyone, It Is not my intention to spam...\n",
      "1932    ******* Facebook is LAME and so 2004! Check ou...\n",
      "1933    Please check out and send to others Freedom an...\n",
      "1934    Nice to meet You - this is Johnny: 1. If You a...\n",
      "1935     hey you ! check out the channel of Alvar Lake !!\n",
      "1936    Hi -this is Johnny: 1. If You already know my ...\n",
      "1940    Check out this video on YouTube:<br />&quot;Th...\n",
      "1942    O peoples of the earth, I have seen how you pe...\n",
      "1945    I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...\n",
      "1946    ********OMG Facebook is OLD! Check out  ------...\n",
      "1947    Hey Music Fans I really appreciate all of you ...\n",
      "1948    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "1949    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "1950    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "Name: content, Length: 1004, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Show spam comments in data\n",
    "print(data_comments[\"content\"][data_comments[\"label\"] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments  1959\n"
     ]
    }
   ],
   "source": [
    "# Add another column with corresponding comment length\n",
    "data_comments['length'] = data_comments['content'].map(lambda text: len(text))\n",
    "\n",
    "#Number of comments\n",
    "print(\"Number of comments \", len(data_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Randomly selecting $75\\%$ of the data as training, and $25\\%$ of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1443.000000\n",
       "mean        0.511435\n",
       "std         0.500043\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed so we get same random allocation on each run of code\n",
    "np.random.seed(0)\n",
    "\n",
    "# Add column vector of randomly generated numbers form U[0,1]\n",
    "data_comments[\"uniform\"] = np.random.uniform(0,1,len(data_comments.index)) \n",
    "\n",
    "# About 75% of these numbers should be less than 0.75\n",
    "data_comments_train = data_comments[data_comments[\"uniform\"] < 0.75]\n",
    "\n",
    "# About 25% of these numbers should be more than 0.75\n",
    "data_comments_test = data_comments[data_comments[\"uniform\"] > 0.75]\n",
    "\n",
    "# Check that both training and test data have both spam and ham comments\n",
    "data_comments_train[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    516.000000\n",
       "mean       0.515504\n",
       "std        0.500245\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data summary statistics\n",
    "data_comments_test[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training data: 5704\n",
      "First 5 words in our unique set of words: \n",
      " ['thundering', 'someone', 'Irish', 'your', 'much!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!check']\n"
     ]
    }
   ],
   "source": [
    "# Join all the comments\n",
    "training_list_words = \"\".join(data_comments_train.iloc[:,0].values)\n",
    "\n",
    "# Split the list of comments into a list of unique words\n",
    "train_unique_words = set(training_list_words.split(' '))\n",
    "\n",
    "# Number of unique words in training \n",
    "vocab_size_train = len(train_unique_words)\n",
    "\n",
    "# Description of comments in training data\n",
    "print('Unique words in training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our unique set of words: \\n % s' % list(train_unique_words)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently \"now!!\" and \"now!!!!\", as well as \"DOES\",\"DoEs\", and \"does\" are all considered to be unique words. For the purposes of spam classification, its probably better to process the data slightly to increase accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in processed training data: 4063\n",
      "First 5 words in our processed unique set of words: \n",
      " ['sooooooooooooong', 'thundering', 'see', 'wish', 'accidental']\n"
     ]
    }
   ],
   "source": [
    "# Only keep letters and numbers\n",
    "train_unique_words = [re.sub(r'[^a-zA-Z0-9]','', words) for words in train_unique_words]\n",
    "\n",
    "# Convert to lower case and get unique set of words\n",
    "train_unique_words = set([words.lower() for words in train_unique_words])\n",
    "\n",
    "# Number of unique words in training \n",
    "vocab_size_train = len(train_unique_words)\n",
    "\n",
    "# Description of summarized comments in training data\n",
    "print('Unique words in processed training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our processed unique set of words: \\n % s' % list(train_unique_words)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equation 1\n",
    "#P(spam/vocab) = (P(vocab/spam)*P(spam))/P(vocab)\n",
    "#P(ham/vocab) = (P(vocab/ham)*P(ham))/P(vocab)\n",
    "\n",
    "#equation2\n",
    "#P(vocab/spam) = P(word1/spam)*P(word2/spam)*P(word3/spam)...\n",
    "#P(vocab/ham) = P(word1/ham)*P(word2/ham)*P(word3/ham)...\n",
    "\n",
    "#equation3\n",
    "#P(word1|spam)=(count of word1 belonging to category spam)/(total count of words belonging to category spam)\n",
    "#P(word1|ham)=(count of word1 belonging to category ham)/(total count of words belonging to category ham)\n",
    "\n",
    "#equation4\n",
    "#P(word1|ham)=\n",
    "#            (count of word1 belonging to category ham +1)/\n",
    "#                          (total count of words belonging to ham + no of distinct words in training data sets)\n",
    "\n",
    "#P(word1|spam)=\n",
    "#            (count of word1 belonging to category spam +1)/\n",
    "#                       (total count of words belonging to spam + no of distinct words in training data sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with comment words as \"keys\", and their label as \"value\"\n",
    "trainPositive = dict()\n",
    "trainNegative = dict()\n",
    "\n",
    "# Intiailize classes\n",
    "positiveTotal = 0\n",
    "negativeTotal = 0\n",
    "\n",
    "# Initialize Prob. of\n",
    "pSpam = 0.0\n",
    "pNotSpam = 0.0\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary of words and their labels   \n",
    "for word in train_unique_words:\n",
    "    \n",
    "    # Classify all words for now as ham (legitimate)\n",
    "    trainPositive[word] = 0\n",
    "    trainNegative[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of times word in comment appear in spam and ham comments\n",
    "def processComment(comment,label):\n",
    "    global positiveTotal\n",
    "    global negativeTotal\n",
    "    \n",
    "    # Split comments into words\n",
    "    comment = comment.split(' ')\n",
    "    \n",
    "    # Go over each word in comment\n",
    "    for word in comment:\n",
    "        \n",
    "        # ham commments\n",
    "        if(label == 0 and word != ' '):\n",
    "            \n",
    "            # Increment number of times word appears in ham comments\n",
    "            trainNegative[word] = trainNegative.get(word,0)+1\n",
    "            negativeTotal += 1\n",
    "            \n",
    "        # spam comments\n",
    "        elif(label == 1 and word != ' '):\n",
    "            \n",
    "            # Increment number of times word appears in spam comments\n",
    "            trainPositive[word] = trainPositive.get(word,0)+1\n",
    "            positiveTotal += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob(word|spam) and Prob(word|ham)\n",
    "def conditionalWord(word,label):\n",
    "    \n",
    "    # Laplace smoothing parameter\n",
    "    global alpha\n",
    "    \n",
    "    # word in ham comment\n",
    "    if(label == 0):\n",
    "        # Compute Prob(word|ham)\n",
    "        return (trainNegative.get(word,0)+alpha)/(float)(negativeTotal+alpha*vocab_size_train)\n",
    "    \n",
    "    # word in spam comment\n",
    "    else:\n",
    "        \n",
    "        # Compute Prob(word|ham)\n",
    "        return (trainPositive.get(word,0)+alpha)/(float)(positiveTotal+alpha*vocab_size_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prob(spam|comment) or Prob(ham|comment)\n",
    "def conditionalComment(comment,label):\n",
    "    \n",
    "    # Initialize conditional probability\n",
    "    prob_label_comment = 1.0\n",
    "    \n",
    "    # Split comments into list of words\n",
    "    comment = comment.split(' ')\n",
    "    \n",
    "    # Go through all words in comments\n",
    "    for word in comment:\n",
    "        \n",
    "        # Compute value proportional to Prob(label|comment)\n",
    "        prob_label_comment *= conditionalWord(word,label)\n",
    "    \n",
    "    return prob_label_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train naive bayes by computing several conditional probabilities in training data\n",
    "def train():\n",
    "    \n",
    "    print('Starting training')\n",
    "    global pSpam\n",
    "    global pNotSpam\n",
    "\n",
    "    # Initiailize \n",
    "    total = 0\n",
    "    numNegative = 0\n",
    "    \n",
    "    # Go over each comment in training data\n",
    "    for idx, comment in data_comments_train.iterrows():\n",
    "        \n",
    "        # Comment is ham \n",
    "        if comment.label == 0:\n",
    "            \n",
    "            # Increment ham comment counter\n",
    "            numNegative += 1\n",
    "        \n",
    "        # Increment comment number\n",
    "        total += 1\n",
    "        \n",
    "        # Update dictionary of ham and spam comments\n",
    "        processComment(comment.content,comment.label)\n",
    "    \n",
    "    # Compute prior probabilities, P(spam), P(ham)\n",
    "    pNotSpam = numNegative/float(total)\n",
    "    pSpam = (total - numNegative)/float(total)\n",
    "    \n",
    "    print('Training is now finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Training is now finished\n"
     ]
    }
   ],
   "source": [
    "# Run naive bayes\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify comment are spam or ham\n",
    "def classify(comment):\n",
    "    \n",
    "    global pSpam\n",
    "    global pNotSpam\n",
    "    \n",
    "    # Compute value proportional to Pr(comment|ham)\n",
    "    isNegative = pSpam * conditionalComment(comment,0)\n",
    "    \n",
    "    # Compute value proportional to Pr(comment|spam)\n",
    "    isPositive = pNotSpam * conditionalComment(comment,1)\n",
    "    \n",
    "    # Output True = spam, False = ham\n",
    "    return (isNegative < isPositive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of comments classified correctly on test set: 0.8565891472868217\n"
     ]
    }
   ],
   "source": [
    "# Initialize spam prediction in test data\n",
    "prediction_test = []\n",
    "\n",
    "# Get prediction accuracy on test data\n",
    "for comment in data_comments_test[\"content\"]:\n",
    "\n",
    "    # Classify comment \n",
    "    prediction_test.append(classify(comment))\n",
    "\n",
    "# Check accuracy\n",
    "test_accuracy = np.mean(np.equal(prediction_test, data_comments_test[\"label\"]))\n",
    "\n",
    "#print prediction_test\n",
    "print(\"Proportion of comments classified correctly on test set:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"True\" is for spam comments, and \"False\" is for ham comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spam\n",
    "classify(\"Guys check out my new chanell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spam\n",
    "classify(\"I have solved P vs. NP, check my video https://www.youtube.com/watch?v=dQw4w9WgXcQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ham\n",
    "classify(\"I liked the video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ham\n",
    "classify(\"Its great that this video has so many views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4885654885654886"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pNotSpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
